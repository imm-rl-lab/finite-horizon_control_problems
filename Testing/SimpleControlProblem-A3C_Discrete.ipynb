{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "vhAdVDYQDKiG"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (410) must match the size of tensor b (82) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-63ba240c8d28>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[0mvisualizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSimpleControlProblemVisualizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwaiting_for_show\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m \u001b[0msolver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgo_asynchronously\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0magent\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepisode_n\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m101\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshow\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvisualizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msession_len\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2500\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msession_n\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Desktop\\IMM-RL-Lab\\finite-horizon_control_problems\\Solvers\\OneAgentSolver.py\u001b[0m in \u001b[0;36mgo_asynchronously\u001b[1;34m(env, agent, show, episode_n, session_n, session_len, agent_learning)\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0magent_learning\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 55\u001b[1;33m             \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_sessions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     56\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m         \u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0magent\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepisode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msession_array\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\IMM-RL-Lab\\finite-horizon_control_problems\\Agents\\Utilities\\AgentTransformations.py\u001b[0m in \u001b[0;36mnew_fit\u001b[1;34m(self, all_sessions)\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mnew_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mall_sessions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0magent\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msessions\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minner_agents\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mall_sessions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 92\u001b[1;33m         \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msessions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     93\u001b[0m         for local_pi_model, local_v_model, global_pi_model, global_v_model in zip(\n\u001b[0;32m     94\u001b[0m                 \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpi_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\IMM-RL-Lab\\finite-horizon_control_problems\\Agents\\Utilities\\ContinuousAgentMakers\\ContinuousAgentMaker.py\u001b[0m in \u001b[0;36mfit_by_sessions\u001b[1;34m(self, sessions)\u001b[0m\n\u001b[0;32m     35\u001b[0m                     \u001b[0mactions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m                 \u001b[0msession\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'actions'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mactions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m             \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msessions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     38\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\IMM-RL-Lab\\finite-horizon_control_problems\\Agents\\A2C.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, sessions)\u001b[0m\n\u001b[0;32m     73\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m         \u001b[1;31m#update pi_model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 75\u001b[1;33m         \u001b[0mpi_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog_prob\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0madvantages\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mentropy_threshold\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mentropy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     76\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpi_optimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m         \u001b[0mpi_loss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The size of tensor a (410) must match the size of tensor b (82) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "sys.path.insert(0, os.path.abspath('..'))\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from Agents.Utilities.SequentialNetwork import SequentialNetwork\n",
    "from Agents.Utilities.AgentTransformations import get_continuous_agent, get_asynchronous_agent\n",
    "from Agents.Utilities.ContinuousAgentMakers.ContinuousAgentMaker import ContinuousAgentMaker\n",
    "from Environments.SimpleControlProblem.SimpleControlProblem import SimpleControlProblem\n",
    "from Environments.SimpleControlProblem.SimpleControlProblemVisualizer import SimpleControlProblemVisualizer\n",
    "import Solvers.OneAgentSolver as solver\n",
    "from Agents.A2C import A2C\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "env = SimpleControlProblem()\n",
    "action_n = 3\n",
    "action_values = np.linspace(env.action_min, env.action_max, action_n).reshape(action_n, 1)\n",
    "\n",
    "A2C = ContinuousAgentMaker(A2C)\n",
    "pi_model = SequentialNetwork([env.state_dim, 128, 1])\n",
    "v_model = SequentialNetwork([env.state_dim, 128, 1])\n",
    "agent = A2C(env.action_min, env.action_max, pi_model, v_model, action_values=action_values,\n",
    "             gamma=1, pi_model_lr=1e-3, v_model_lr=1e-3, entropy_threshold=0.01, action_space_type='discrete')\n",
    "\n",
    "agent = get_asynchronous_agent(agent, inner_agent_n=10)\n",
    "\n",
    "visualizer = SimpleControlProblemVisualizer(waiting_for_show=10)\n",
    "solver.go_asynchronously(env, agent, episode_n=101, show=visualizer.show, session_len=2500, session_n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Untitled3.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
